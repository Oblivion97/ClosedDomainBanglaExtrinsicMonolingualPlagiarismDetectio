{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#...........import Bangla stopword................\n",
    "df = pd.read_excel('bng_sw.xlsx', sheet_name=0)\n",
    "bng_stopword = df['Stopwords'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['বই পড়লে জানা যায় নানা কিছু', ' আমি তো প্রচুর বই পড়ি', ' অনেক কিছু জানা যায় বই পড়লে', ' বই পড়তে গেলে নানা চিন্তাভাবনা করতে হয়, মনে রাখতে হয় নানা কিছু']\n"
     ]
    }
   ],
   "source": [
    "#............read article.................\n",
    "f = open(\"doc1.txt\", encoding = 'utf-8')\n",
    "text = f.read()\n",
    "# print(\"Test Data Is: \",text)\n",
    "\n",
    "# .........split every sentence & store an array............\n",
    "sentence1 = text.split(\"।\")\n",
    "np_sent1 = np.array(sentence1)\n",
    "\n",
    "# .........remove last row for blank line...........\n",
    "np_sent1 = np.delete(np_sent1,-1)\n",
    "\n",
    "#.........Store original sentences in an array.......\n",
    "org_sent = []\n",
    "for sentence in np_sent1:\n",
    "    org_sent.append(sentence)\n",
    "print(org_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopword(sentence_array):\n",
    "    for i in range(len(sentence_array)):\n",
    "        text_tokens = word_tokenize(sentence_array[i])\n",
    "        tokens_without_sw = [word for word in text_tokens if not word in bng_stopword]\n",
    "        filtered_text = (\" \").join(tokens_without_sw)\n",
    "        sentence_array[i] = filtered_text\n",
    "\n",
    "#     print(\"After Remove Stop-Word:\\n\", np_sent1)\n",
    "    return(sentence_array)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_actual_token(sentence_array):\n",
    "    actual_tokens = []\n",
    "    for i in range(len(sentence_array)):\n",
    "        text_tokens = word_tokenize(sentence_array[i])\n",
    "        for j in range(len(text_tokens)):\n",
    "            actual_tokens.append(text_tokens[j])\n",
    "\n",
    "    actual_tokens = list(dict.fromkeys(actual_tokens))\n",
    "    return actual_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countX(lst, x): \n",
    "    count = 0\n",
    "    for ele in lst: \n",
    "        if (ele == x): \n",
    "            count = count + 1\n",
    "    return count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_TF_score(sent_array,token):\n",
    "    rows, cols = (len(sent_array), len(token)) \n",
    "    tf_score = [[0 for i in range(cols)] for j in range(rows)]\n",
    "\n",
    "    for i in range(rows):\n",
    "        text_tokens = word_tokenize(sent_array[i])\n",
    "        count = 0\n",
    "        total_term = len(text_tokens)\n",
    "        for j in range(cols):\n",
    "            rep_term = countX(text_tokens,token[j])\n",
    "            count = (rep_term/total_term);\n",
    "            formatted_float = \"{:.5f}\".format(count)\n",
    "            tf_score[i][j] = formatted_float\n",
    "            \n",
    "    return tf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_IDF_score(sent_array,token):\n",
    "    idf_score = []\n",
    "\n",
    "    for i in range(len(token)):\n",
    "        count_idf = 0\n",
    "        score = 0\n",
    "        for j in range(len(sent_array)):\n",
    "            sentence = word_tokenize(sent_array[j])\n",
    "            term = token[i]\n",
    "            for ele in sentence:\n",
    "                if (ele == term):\n",
    "                    count_idf = count_idf + 1\n",
    "                    break\n",
    "        score = math.log(len(sent_array)/count_idf,10)\n",
    "        formatted_float = \"{:.5f}\".format(score)\n",
    "        idf_score.append(formatted_float)\n",
    "    \n",
    "    return idf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_TF_IDF_score(np_sentences,token_list):\n",
    "    rows, cols = (len(np_sentences), len(token_list)) \n",
    "    tf_idf_score = [[0 for i in range(cols)] for j in range(rows)]\n",
    "    \n",
    "    #........Count TF score of each sentence..............\n",
    "    tf_score = count_TF_score(np_sentences,token_list)\n",
    "    \n",
    "    #........Count IDF score of each sentence..............\n",
    "    idf_score = count_IDF_score(np_sentences,token_list)\n",
    "\n",
    "    for i in range (rows):\n",
    "        for j in range(cols):\n",
    "            tf = tf_score[i][j]\n",
    "            idf = idf_score[j]\n",
    "            score = float(tf) * float(idf)\n",
    "            formatted_float = \"{:.5f}\".format(score)\n",
    "            tf_idf_score[i][j] = formatted_float\n",
    "           \n",
    "    return tf_idf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.........Remove Stop-Word From Each Sentence........\n",
    "filtered_sentence = remove_stopword(np_sent1)\n",
    "# print(filtered_sentence)\n",
    "\n",
    "#........Count actual token of full article..........\n",
    "actual_tokens = count_actual_token(filtered_sentence)\n",
    "# print(actual_tokens)\n",
    "\n",
    "#........Count TF-IDF score of each sentence..........\n",
    "tf_idf_vector = count_TF_IDF_score(filtered_sentence,actual_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  বই পড়লে জানা যায় নানা কিছু\n",
      "Vector:  ['0.00000', '0.15052', '0.00000', '0.00000', '0.00000', '0.00000', '0.00000', '0.00000', '0.00000']\n",
      "Sentence:   আমি তো প্রচুর বই পড়ি\n",
      "Vector:  ['0.00000', '0.00000', '0.20068', '0.20068', '0.00000', '0.00000', '0.00000', '0.00000', '0.00000']\n",
      "Sentence:   অনেক কিছু জানা যায় বই পড়লে\n",
      "Vector:  ['0.00000', '0.15052', '0.00000', '0.00000', '0.00000', '0.00000', '0.00000', '0.00000', '0.00000']\n",
      "Sentence:   বই পড়তে গেলে নানা চিন্তাভাবনা করতে হয়, মনে রাখতে হয় নানা কিছু\n",
      "Vector:  ['0.00000', '0.00000', '0.00000', '0.00000', '0.10035', '0.10035', '0.10035', '0.10035', '0.10035']\n"
     ]
    }
   ],
   "source": [
    "#.......Print Output.........\n",
    "for i in range(len(org_sent)):\n",
    "    print(\"Sentence: \", org_sent[i])\n",
    "    print(\"Vector: \", tf_idf_vector[i][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
