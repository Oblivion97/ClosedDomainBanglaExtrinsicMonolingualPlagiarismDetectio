{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import errno\n",
    "from os.path import isfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data Is:  ﻿ভালো-মন্দের মধ্য দিয়েই ২০১৯ সাল পার করল বাংলাদেশ। সারা বছর ধরেই নানা আন্তর্জাতিক সংস্থা বিভিন্ন সূচক প্রকাশ করে থাকে। সার্বিকভাবে এসব সূচক দিয়ে বহির্বিশ্বে বাংলাদেশের অবস্থান নির্ধারণ করা হয়। তবে আশঙ্কার বিষয় হলো কিছু সূচকে এত দিন ইতিবাচক ধারায় থাকলেও এখন চ্যালেঞ্জের মুখে পড়ছে। ১৬০ দেশের মধ্যে বাংলাদেশ ১৩৪তম। \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#....test data...\n",
    "f = open(\"test.txt\", encoding = 'utf-8')\n",
    "a2 = f.read()\n",
    "\n",
    "print(\"Test Data Is: \",a2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeffভালো-মন্দের মধ্য দিয়েই ২০১৯ সাল পার করল বাংলাদেশ'\n",
      " ' সারা বছর ধরেই নানা আন্তর্জাতিক সংস্থা বিভিন্ন সূচক প্রকাশ করে থাকে'\n",
      " ' সার্বিকভাবে এসব সূচক দিয়ে বহির্বিশ্বে বাংলাদেশের অবস্থান নির্ধারণ করা হয়'\n",
      " ' তবে আশঙ্কার বিষয় হলো কিছু সূচকে এত দিন ইতিবাচক ধারায় থাকলেও এখন চ্যালেঞ্জের মুখে পড়ছে'\n",
      " ' ১৬০ দেশের মধ্যে বাংলাদেশ ১৩৪তম' ' \\n']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np_sent1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-019d2a6c1a12>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#......vectorization.........\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mtfidf_matrix_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp_sent1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;31m#print(tfidf_matrix_train[:]);\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np_sent1' is not defined"
     ]
    }
   ],
   "source": [
    "sentence2 = a2.split(\"।\")\n",
    "np_sent2 = np.array(sentence2)\n",
    "len_np_sent2 = len(np_sent2)-1\n",
    "\n",
    "#.....create a array for mark text sent is it copy....\n",
    "mark_line = np.zeros(len_np_sent2)\n",
    "print(np_sent2)\n",
    "\n",
    "#......vectorization.........\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix_train = tfidf_vectorizer.fit_transform(np_sent1)\n",
    "#print(tfidf_matrix_train[:]);\n",
    "print(tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence2 = a2.split(\"।\")\n",
    "np_sent2 = np.array(sentence2)\n",
    "len_np_sent2 = len(np_sent2)-1\n",
    "\n",
    "#.....create a array for mark text sent is it copy....\n",
    "mark_line = np.zeros(len_np_sent2)\n",
    "\n",
    "path = 'C:/Users/Adil_Ahnaf/Downloads/Cosine Similarity_V3.1/train data'\n",
    "\n",
    "files=filter(isfile,glob.glob('%s/*'%path))\n",
    "\n",
    "for name in files:\n",
    "        f = open(name, encoding = 'utf-8')\n",
    "        name = str(name).replace('C:/Users/Adil_Ahnaf/Downloads/Cosine Similarity_V3.1/train data', '')\n",
    "        print(\"\\n Train Doc iS: \",name)\n",
    "        a1 = f.read()\n",
    "#         print(a1)\n",
    "        f.close()\n",
    "        \n",
    "        # ......split every sentence & store an array\n",
    "        sentence1 = a1.split(\"।\")\n",
    "        sentence2 = a2.split(\"।\")\n",
    "\n",
    "        # ......store each document in different array\n",
    "        np_sent1 = np.array(sentence1)\n",
    "        np_sent2 = np.array(sentence2)\n",
    "\n",
    "        # ......remove last row for blank line\n",
    "        len_np_sent1 = len(np_sent1)-1\n",
    "        en_np_sent2 = len(np_sent2)-1\n",
    "\n",
    "        #print(len_np_sent1,len_np_sent2)\n",
    "        \n",
    "        #print(\"train data:\", np_sent1)\n",
    "        \n",
    "        np_sent1 = np.delete(np_sent1,len_np_sent1)\n",
    "        np_sent2 = np.delete(np_sent2,len_np_sent2)\n",
    "\n",
    "        # .....store values os np_sent2 in np_sent1\n",
    "        np_sent1 = np.concatenate((np_sent1[:],np_sent2[:]))\n",
    "    \n",
    "        #print(\"mark line:\",mark_line)\n",
    "        #print(\"\\n Total Sentences: \",np_sent1)\n",
    "        #print(len(np_sent1))\n",
    "        \n",
    "        #........vectorize every sentence using TF-IDF\n",
    "        tfidf_vectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix_train = tfidf_vectorizer.fit_transform(np_sent1)\n",
    "        #print(tfidf_matrix_train[:]);\n",
    "        \n",
    "        #..........run cosine similarity algorithms.........\n",
    "        flag1 = len_np_sent1\n",
    "        flag2 = flag1 + 1\n",
    "        count_copy = 0\n",
    "        x = 0\n",
    "#         print(flag1,flag2)\n",
    "#        print(\"mark line:\",mark_line)\n",
    "        \n",
    "        while flag1 != len(np_sent1):\n",
    "            cosine_array = np.array(cosine_similarity(tfidf_matrix_train[flag1:flag2], tfidf_matrix_train))\n",
    "            #print(\"cosine scores ==> \",cosine_array)\n",
    "            \n",
    "        #......identify test data sentance index.........\n",
    "#             print(\"Testing index\")\n",
    "#             print(\"falg2\",flag2-1,\"len1\",len_np_sent1-1)\n",
    "#             mark_index = ((flag2-1)-(len_np_sent1-1))-1\n",
    "#             print(\"index is:\" , mark_index)\n",
    "            \n",
    "            flag1 = flag2\n",
    "            flag2 = flag1 + 1\n",
    "            result = cosine_array.flatten()\n",
    "            result = result[:-len_np_sent2]\n",
    "\n",
    "            #.........remove test data result.....\n",
    "\n",
    "#             print(\"length:\",len(result))\n",
    "#             print(\"1D is:\",result)\n",
    "            max_value = np.amax(result)\n",
    "            #print('Max Value from  Array : ', max_value)\n",
    "            if (max_value >= 0.90 and mark_line[x]== 0):\n",
    "                print(\"Copied Line is:\",np_sent1[flag1-1:flag2-1])\n",
    "                mark_line[x] = 1\n",
    "                x = x+1\n",
    "#                 print(\"mark line:\",mark_line,\"\\n\")\n",
    "                count_copy = count_copy + 1\n",
    "            else:\n",
    "                x = x+1 \n",
    "\n",
    "        print(\"No of Lines copied:\" , count_copy)\n",
    "        \n",
    "        #......Calculate  plagiarism in with train doc..........\n",
    "        total_line = len_np_sent2\n",
    "        copied_line = count_copy\n",
    "        plagiarism = (copied_line/total_line)*100\n",
    "        print(\"Plagiarism Result is: \",plagiarism,\"%\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#........calculate total plagiarism of test data...........\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Plagiarism Found :  0.0 %\n"
     ]
    }
   ],
   "source": [
    "total_line = len_np_sent2\n",
    "copied_line = 0\n",
    "\n",
    "for x in range(len(mark_line)):\n",
    "    if mark_line[x] == 1:\n",
    "        copied_line = copied_line + 1\n",
    "        \n",
    "plagiarism = (copied_line/total_line)*100\n",
    "print(\"Total Plagiarism Found : \",plagiarism,\"%\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
