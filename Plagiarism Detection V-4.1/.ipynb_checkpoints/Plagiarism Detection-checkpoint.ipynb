{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import re\n",
    "import glob\n",
    "import errno\n",
    "from os.path import isfile\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from docx.shared import RGBColor\n",
    "from docx import Document\n",
    "from docx.text.run import Font, Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#...........import Bangla stopword................\n",
    "df = pd.read_excel('bng_sw.xlsx', sheet_name=0)\n",
    "bng_stopword = df['Stopwords'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopword(sentence_array):\n",
    "    for i in range(len(sentence_array)):\n",
    "        text_tokens = word_tokenize(sentence_array[i])\n",
    "        tokens_without_sw = [word for word in text_tokens if not word in bng_stopword]\n",
    "        filtered_text = (\" \").join(tokens_without_sw)\n",
    "        sentence_array[i] = filtered_text\n",
    "\n",
    "#     print(\"After Remove Stop-Word:\\n\", np_sent1)\n",
    "    return(sentence_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_actual_token(sentence_array):\n",
    "    actual_tokens = []\n",
    "    for i in range(len(sentence_array)):\n",
    "        text_tokens = word_tokenize(sentence_array[i])\n",
    "        for j in range(len(text_tokens)):\n",
    "            actual_tokens.append(text_tokens[j])\n",
    "\n",
    "    actual_tokens = list(dict.fromkeys(actual_tokens))\n",
    "    return actual_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countX(lst, x): \n",
    "    count = 0\n",
    "    for ele in lst: \n",
    "        if (ele == x): \n",
    "            count = count + 1\n",
    "    return count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_TF_score(sent_array,token):\n",
    "    rows, cols = (len(sent_array), len(token)) \n",
    "    tf_score = [[0 for i in range(cols)] for j in range(rows)]\n",
    "\n",
    "    for i in range(rows):\n",
    "        text_tokens = word_tokenize(sent_array[i])\n",
    "        count = 0\n",
    "        total_term = len(text_tokens)\n",
    "        for j in range(cols):\n",
    "            rep_term = countX(text_tokens,token[j])\n",
    "            count = (rep_term/total_term);\n",
    "            formatted_float = \"{:.5f}\".format(count)\n",
    "            tf_score[i][j] = formatted_float\n",
    "            \n",
    "    return tf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_IDF_score(sent_array,token):\n",
    "    idf_score = []\n",
    "\n",
    "    for i in range(len(token)):\n",
    "        count_idf = 0\n",
    "        score = 0\n",
    "        for j in range(len(sent_array)):\n",
    "            sentence = word_tokenize(sent_array[j])\n",
    "            term = token[i]\n",
    "            for ele in sentence:\n",
    "                if (ele == term):\n",
    "                    count_idf = count_idf + 1\n",
    "                    break\n",
    "        score = math.log(len(sent_array)/count_idf,10)\n",
    "        formatted_float = \"{:.5f}\".format(score)\n",
    "        idf_score.append(formatted_float)\n",
    "    \n",
    "    return idf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_TF_IDF_score(np_sentences,token_list):\n",
    "    rows, cols = (len(np_sentences), len(token_list)) \n",
    "    tf_idf_score = [[0 for i in range(cols)] for j in range(rows)]\n",
    "    \n",
    "    #........Count TF score of each sentence..............\n",
    "    tf_score = count_TF_score(np_sentences,token_list)\n",
    "    \n",
    "    #........Count IDF score of each sentence..............\n",
    "    idf_score = count_IDF_score(np_sentences,token_list)\n",
    "\n",
    "    for i in range (rows):\n",
    "        for j in range(cols):\n",
    "            tf = tf_score[i][j]\n",
    "            idf = idf_score[j]\n",
    "            score = float(tf) * float(idf)\n",
    "            formatted_float = \"{:.5f}\".format(score)\n",
    "            tf_idf_score[i][j] = formatted_float\n",
    "           \n",
    "    return tf_idf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['২০১৯ সাল বাংলাদেশ অনেক ভালো-মন্দের মধ্য দিয়েই পার করল', ' সারা বছর ধরেই নানা আন্তর্জাতিক সংস্থা বিভিন্ন সূচক প্রকাশ করে থাকে', ' ওই সব সূচকে বাংলাদেশ আগের চেয়ে কয়েকটিতে ভালো করেছে, কয়েকটি সূচকে খারাপ করেছে', ' ভালো-মন্দের মধ্য দিয়েই ২০১৯ সাল পার করল বাংলাদেশ']\n",
      "\n",
      " Train Doc iS:  \\doc1.txt\n",
      "Max Value from  Array :  0.9999999999999998\n",
      "Copied Line is: ২০১৯ সাল বাংলাদেশ অনেক ভালো-মন্দের মধ্য দিয়েই পার করল\n",
      "Max Value from  Array :  1.0\n",
      "Copied Line is:  সারা বছর ধরেই নানা আন্তর্জাতিক সংস্থা বিভিন্ন সূচক প্রকাশ করে থাকে\n",
      "Max Value from  Array :  0.9999999999999998\n",
      "Copied Line is:  ওই সব সূচকে বাংলাদেশ আগের চেয়ে কয়েকটিতে ভালো করেছে, কয়েকটি সূচকে খারাপ করেছে\n",
      "Max Value from  Array :  0.9999999999999998\n",
      "Copied Line is:  ভালো-মন্দের মধ্য দিয়েই ২০১৯ সাল পার করল বাংলাদেশ\n",
      "No of Lines copied: 4\n",
      "Plagiarism Result is:  100.0 %\n"
     ]
    }
   ],
   "source": [
    "document = Document()\n",
    "\n",
    "#...........Read Test Data..................\n",
    "f = open(\"test2.txt\", encoding = 'utf-8')\n",
    "a2 = f.read()\n",
    "# print(\"Test Data Is: \",text)\n",
    "\n",
    "# .........split every sentence & store an array............\n",
    "sentence2 = a2.split(\"।\")\n",
    "np_sent2 = np.array(sentence2)\n",
    "\n",
    "# .........remove last row for blank line...........\n",
    "np_sent2 = np.delete(np_sent2,-1)\n",
    "\n",
    "#.........Store original sentences in an array.......\n",
    "org_sent = []\n",
    "for sentence in np_sent2:\n",
    "    org_sent.append(sentence)\n",
    "print(org_sent)\n",
    "\n",
    "#.....create a array for mark text sent is it copy....\n",
    "mark_line = np.zeros(len(np_sent2))\n",
    "# print(mark_line)\n",
    "\n",
    "path = 'C:/Users/DOLPHIN/Downloads/Plagiarism Detection V-0.1/train data2'\n",
    "\n",
    "files=filter(isfile,glob.glob('%s/*'%path))\n",
    "\n",
    "for name in files:\n",
    "        f = open(name, encoding = 'utf-8')\n",
    "        name = str(name).replace('C:/Users/DOLPHIN/Downloads/Plagiarism Detection V-0.1/train data2', '')\n",
    "        print(\"\\n Train Doc iS: \",name)\n",
    "        a1 = f.read()\n",
    "#         print(a1)\n",
    "        f.close()\n",
    "        \n",
    "        # ......split every sentence & store an array\n",
    "        sentence1 = a1.split(\"।\")\n",
    "        sentence2 = a2.split(\"।\")\n",
    "\n",
    "        # ......store each document in different array\n",
    "        np_sent1 = np.array(sentence1)\n",
    "        np_sent2 = np.array(sentence2)\n",
    "\n",
    "        # ......remove last row for blank line\n",
    "        len_np_sent1 = len(np_sent1)-1\n",
    "        len_np_sent2 = len(np_sent2)-1\n",
    "\n",
    "        #print(len_np_sent1,len_np_sent2)\n",
    "        \n",
    "        #print(\"train data:\", np_sent1)\n",
    "        \n",
    "        np_sent1 = np.delete(np_sent1,len_np_sent1)\n",
    "        np_sent2 = np.delete(np_sent2,len_np_sent2)\n",
    "\n",
    "        # .....store values os np_sent2 in np_sent1\n",
    "        np_sent1 = np.concatenate((np_sent1[:],np_sent2[:]))\n",
    "    \n",
    "        #print(\"mark line:\",mark_line)\n",
    "        #print(\"\\n Total Sentences: \",np_sent1)\n",
    "        #print(len(np_sent1))\n",
    "        \n",
    "        #.........Remove Stop-Word From Each Sentence........\n",
    "        filtered_sentence = remove_stopword(np_sent1)\n",
    "        # print(filtered_sentence)\n",
    "\n",
    "        #........Count actual token of full article..........\n",
    "        actual_tokens = count_actual_token(filtered_sentence)\n",
    "        # print(actual_tokens)\n",
    "\n",
    "        #........Count TF-IDF score of each sentence..........\n",
    "        tfidf_matrix_train = count_TF_IDF_score(filtered_sentence,actual_tokens)\n",
    "        #print(tfidf_matrix_train[:]);\n",
    "        \n",
    "        #..........run cosine similarity algorithms.........\n",
    "        flag1 = len_np_sent1\n",
    "        flag2 = flag1 + 1\n",
    "        count_copy = 0\n",
    "        x = 0\n",
    "#         print(flag1,flag2)\n",
    "#        print(\"mark line:\",mark_line)\n",
    "        \n",
    "        while flag1 != len(np_sent1):\n",
    "            cosine_array = np.array(cosine_similarity(tfidf_matrix_train[flag1:flag2], tfidf_matrix_train))\n",
    "            #print(\"cosine scores ==> \",cosine_array)\n",
    "            \n",
    "        #......identify test data sentance index.........\n",
    "#             print(\"Testing index\")\n",
    "#             print(\"falg2\",flag2-1,\"len1\",len_np_sent1-1)\n",
    "#             mark_index = ((flag2-1)-(len_np_sent1-1))-1\n",
    "#             print(\"index is:\" , mark_index)\n",
    "            \n",
    "            flag1 = flag2\n",
    "            flag2 = flag1 + 1\n",
    "            result = cosine_array.flatten()\n",
    "            result = result[:-len_np_sent2]\n",
    "\n",
    "            #.........remove test data result.....\n",
    "\n",
    "#             print(\"length:\",len(result))\n",
    "#             print(\"1D is:\",result)\n",
    "            max_value = np.amax(result)\n",
    "            print('Max Value from  Array : ', max_value)\n",
    "            if (max_value >= 0.90 and mark_line[x]== 0):\n",
    "                #print(\"Copied Line is:\", org_sent[x])\n",
    "                run = document.add_paragraph().add_run(org_sent[x])\n",
    "                mark_line[x] = 1\n",
    "                x = x+1\n",
    "#                 print(\"mark line:\",mark_line,\"\\n\")\n",
    "                count_copy = count_copy + 1\n",
    "            else:\n",
    "                x = x+1\n",
    "                run = document.add_paragraph().add_run(org_sent[x])\n",
    "\n",
    "        print(\"No of Lines copied:\" , count_copy)\n",
    "        document.save('report.docx')\n",
    "        #......Calculate  plagiarism in with train doc..........\n",
    "        total_line = len_np_sent2\n",
    "        copied_line = count_copy\n",
    "        plagiarism = (copied_line/total_line)*100\n",
    "        print(\"Plagiarism Result is: \",plagiarism,\"%\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Plagiarism Found :  100.0 %\n"
     ]
    }
   ],
   "source": [
    "#........calculate total plagiarism of test data...........\n",
    "total_line = len_np_sent2\n",
    "copied_line = 0\n",
    "\n",
    "for x in range(len(mark_line)):\n",
    "    if mark_line[x] == 1:\n",
    "        copied_line = copied_line + 1\n",
    "        \n",
    "plagiarism = (copied_line/total_line)*100\n",
    "print(\"Total Plagiarism Found : \",plagiarism,\"%\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
