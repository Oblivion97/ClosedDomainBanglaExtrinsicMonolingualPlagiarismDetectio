{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import re\n",
    "import glob\n",
    "import errno\n",
    "from os.path import isfile\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from docx.shared import RGBColor\n",
    "from docx import Document\n",
    "from docx.text.run import Font, Run\n",
    "import docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#...........import Bangla stopword................\n",
    "df = pd.read_excel('bng_sw.xlsx', sheet_name=0)\n",
    "bng_stopword = df['Stopwords'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopword(sentence_array):\n",
    "    for i in range(len(sentence_array)):\n",
    "        text_tokens = word_tokenize(sentence_array[i])\n",
    "        tokens_without_sw = [word for word in text_tokens if not word in bng_stopword]\n",
    "        filtered_text = (\" \").join(tokens_without_sw)\n",
    "        sentence_array[i] = filtered_text\n",
    "\n",
    "#     print(\"After Remove Stop-Word:\\n\", np_sent1)\n",
    "    return(sentence_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_actual_token(sentence_array):\n",
    "    actual_tokens = []\n",
    "    for i in range(len(sentence_array)):\n",
    "        text_tokens = word_tokenize(sentence_array[i])\n",
    "        for j in range(len(text_tokens)):\n",
    "            actual_tokens.append(text_tokens[j])\n",
    "\n",
    "    actual_tokens = list(dict.fromkeys(actual_tokens))\n",
    "    return actual_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countX(lst, x): \n",
    "    count = 0\n",
    "    for ele in lst: \n",
    "        if (ele == x): \n",
    "            count = count + 1\n",
    "    return count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_TF_score(sent_array,token):\n",
    "    rows, cols = (len(sent_array), len(token)) \n",
    "    tf_score = [[0 for i in range(cols)] for j in range(rows)]\n",
    "\n",
    "    for i in range(rows):\n",
    "        text_tokens = word_tokenize(sent_array[i])\n",
    "        count = 0\n",
    "        total_term = len(text_tokens)\n",
    "        for j in range(cols):\n",
    "            rep_term = countX(text_tokens,token[j])\n",
    "            count = (rep_term/total_term);\n",
    "            formatted_float = \"{:.5f}\".format(count)\n",
    "            tf_score[i][j] = formatted_float\n",
    "            \n",
    "    return tf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_IDF_score(sent_array,token):\n",
    "    idf_score = []\n",
    "\n",
    "    for i in range(len(token)):\n",
    "        count_idf = 0\n",
    "        score = 0\n",
    "        for j in range(len(sent_array)):\n",
    "            sentence = word_tokenize(sent_array[j])\n",
    "            term = token[i]\n",
    "            for ele in sentence:\n",
    "                if (ele == term):\n",
    "                    count_idf = count_idf + 1\n",
    "                    break\n",
    "        score = math.log(len(sent_array)/count_idf,10)\n",
    "        formatted_float = \"{:.5f}\".format(score)\n",
    "        idf_score.append(formatted_float)\n",
    "    \n",
    "    return idf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_TF_IDF_score(np_sentences,token_list):\n",
    "    rows, cols = (len(np_sentences), len(token_list)) \n",
    "    tf_idf_score = [[0 for i in range(cols)] for j in range(rows)]\n",
    "    \n",
    "    #........Count TF score of each sentence..............\n",
    "    tf_score = count_TF_score(np_sentences,token_list)\n",
    "    \n",
    "    #........Count IDF score of each sentence..............\n",
    "    idf_score = count_IDF_score(np_sentences,token_list)\n",
    "\n",
    "    for i in range (rows):\n",
    "        for j in range(cols):\n",
    "            tf = tf_score[i][j]\n",
    "            idf = idf_score[j]\n",
    "            score = float(tf) * float(idf)\n",
    "            formatted_float = \"{:.5f}\".format(score)\n",
    "            tf_idf_score[i][j] = formatted_float\n",
    "           \n",
    "    return tf_idf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Test Article Is: '\n",
      " 'অনেকেই ভাবে, যার মস্তিষ্ক যত বড়, তার চিন্তা করার শক্তি তত বেশি'\n",
      " ' ধারণাটি কিন্তু ভুল' ' মানুষের থেকে তিমির মস্তিষ্ক অনেক বড়'\n",
      " ' কিন্তু তিমির থেকে মানুষ বেশি বুদ্ধিমান'\n",
      " ' নেতৃত্ব, জ্ঞান–বিজ্ঞান, সাহিত্যচর্চা—এসব বিষয়ে মানুষ উন্নতি করতে পেরেছে'\n",
      " ' বাংলা ব্যঞ্জনসন্ধির সূত্র মুখস্থ করতে করতে অনেকে অস্থির হয়ে যায়'\n",
      " ' সমীভবন হলো এমন এক প্রক্রিয়া, যেখানে এক ধ্বনির প্রভাবে পাশের ধ্বনিটি বদলে যায়'\n",
      " ' আমার ছোট ফুফুর বাড়ি' ' বহুদিন পর ফুফুর বাড়ি বেড়াতে এলাম'\n",
      " ' হাঁটতে ভালো লাগছে' ' মাঠ পেরোনোর আগেই সন্ধ্যা হয়ে গেল'\n",
      " ' খোলা জায়গায় অন্ধকার জমতে সময় লাগে'\n",
      " ' সূর্য ডোবার পরও অনেকটাক্ষণ আলোকিত ভাব থাকে'\n",
      " '  অন্ধকারে ভয় পাওয়ার কিছু নেই' ' সঙ্গে টর্চ নেই, মোবাইল তো আছে'\n",
      " ' মোবাইলই এখন টর্চের কাজ দেয়'\n",
      " ' ২৪০ পাতার বইটিকে নিছক ভুয়ো বলে মানতে রাজি নন বিশেষজ্ঞরা'\n",
      " ' উর্দু ডানদিকে লেখা হয়েছে বইটি'\n",
      " ' ভেষজ বা আয়ুর্বেদিক চিকিৎসাবিদ্যার নানা দিক নিয়ে বিস্তৃত আলোচনা রয়েছে বইয়ে যা সত্যিই অবাক করে'\n",
      " ' সম্ভবত ১৯১২ সালে বইটি কিনে নেন বইয়ের ব্যবসা করা উইলফ্রিড ভয়নিচ'\n",
      " ' তার নামেই বইটির এই নামকরণ হয়েছে'\n",
      " ' এই মতিভ্রম ওর কেন হলো, কিছুই বুঝতে পারছি না'\n",
      " ' দ্বীপ জুড়ে নেই কোনো গাছ বা লতা-গুল্ম']\n"
     ]
    }
   ],
   "source": [
    "#...........Read Test Data..................\n",
    "f = open(\"Test_data_1.txt\", encoding = 'utf-8')\n",
    "a2 = f.read()\n",
    "# print(\"Test Data Is: \",text)\n",
    "\n",
    "# .........split every sentence & store an array............\n",
    "sentence2 = a2.split(\"।\")\n",
    "np_sent2 = np.array(sentence2)\n",
    "\n",
    "# .........remove last row for blank line...........\n",
    "np_sent2 = np.delete(np_sent2,-1)\n",
    "\n",
    "#.........Store original sentences in an array.......\n",
    "org_sent = []\n",
    "for sentence in np_sent2:\n",
    "    org_sent.append(sentence)\n",
    "    \n",
    "org_sent = np.insert(org_sent,0,'Test Article Is: ')\n",
    "print(org_sent)\n",
    "\n",
    "#........Mark orginal sentence index...........\n",
    "mark_org_index = np.zeros(len(org_sent))\n",
    "\n",
    "#.....create a array for mark text sent is it copy....\n",
    "mark_line = np.zeros(len(np_sent2))\n",
    "# print(mark_line)\n",
    "\n",
    "path = 'C:/Users/DOLPHIN/Downloads/Plagiarism Detection V-4.3/Train_Data'\n",
    "\n",
    "files=filter(isfile,glob.glob('%s/*'%path))\n",
    "\n",
    "#.........Declear some variable..........\n",
    "book_name = []\n",
    "plagiarism_percent = []\n",
    "copy_line = []\n",
    "\n",
    "for name in files:\n",
    "        f = open(name, encoding = 'utf-8')\n",
    "        name = str(name).replace('C:/Users/DOLPHIN/Downloads/Plagiarism Detection V-4.3/Train_Data', '')\n",
    "        name=name.replace(\"\\\\\",\"\")\n",
    "#         print(\"\\n Train Doc iS: \",name)\n",
    "        book_name.append(name)\n",
    "        a1 = f.read()\n",
    "#         print(a1)\n",
    "        f.close()\n",
    "        \n",
    "        # ......split every sentence & store an array\n",
    "        sentence1 = a1.split(\"।\")\n",
    "        sentence2 = a2.split(\"।\")\n",
    "\n",
    "        # ......store each document in different array\n",
    "        np_sent1 = np.array(sentence1)\n",
    "        np_sent2 = np.array(sentence2)\n",
    "\n",
    "        # ......remove last row for blank line\n",
    "        len_np_sent1 = len(np_sent1)-1\n",
    "        len_np_sent2 = len(np_sent2)-1\n",
    "\n",
    "        #print(len_np_sent1,len_np_sent2)\n",
    "        \n",
    "        #print(\"train data:\", np_sent1)\n",
    "        \n",
    "        np_sent1 = np.delete(np_sent1,len_np_sent1)\n",
    "        np_sent2 = np.delete(np_sent2,len_np_sent2)\n",
    "\n",
    "        # .....store values os np_sent2 in np_sent1\n",
    "        np_sent1 = np.concatenate((np_sent1[:],np_sent2[:]))\n",
    "    \n",
    "        #print(\"mark line:\",mark_line)\n",
    "        #print(\"\\n Total Sentences: \",np_sent1)\n",
    "        #print(len(np_sent1))\n",
    "        \n",
    "        #.........Remove Stop-Word From Each Sentence........\n",
    "        filtered_sentence = remove_stopword(np_sent1)\n",
    "        # print(filtered_sentence)\n",
    "\n",
    "        #........Count actual token of full article..........\n",
    "        actual_tokens = count_actual_token(filtered_sentence)\n",
    "        # print(actual_tokens)\n",
    "\n",
    "        #........Count TF-IDF score of each sentence..........\n",
    "        tfidf_matrix_train = count_TF_IDF_score(filtered_sentence,actual_tokens)\n",
    "        #print(tfidf_matrix_train[:]);\n",
    "        \n",
    "        #..........run cosine similarity algorithms.........\n",
    "        flag1 = len_np_sent1\n",
    "        flag2 = flag1 + 1\n",
    "        count_copy = 0\n",
    "        x = 0\n",
    "#         print(flag1,flag2)\n",
    "#        print(\"mark line:\",mark_line)\n",
    "        \n",
    "        while flag1 != len(np_sent1):\n",
    "            cosine_array = np.array(cosine_similarity(tfidf_matrix_train[flag1:flag2], tfidf_matrix_train))\n",
    "            #print(\"cosine scores ==> \",cosine_array)\n",
    "            \n",
    "        #......identify test data sentance index.........\n",
    "#             print(\"Testing index\")\n",
    "#             print(\"falg2\",flag2-1,\"len1\",len_np_sent1-1)\n",
    "#             mark_index = ((flag2-1)-(len_np_sent1-1))-1\n",
    "#             print(\"index is:\" , mark_index)\n",
    "            \n",
    "            flag1 = flag2\n",
    "            flag2 = flag1 + 1\n",
    "            result = cosine_array.flatten()\n",
    "            result = result[:-len_np_sent2]\n",
    "\n",
    "            #.........remove test data result.....\n",
    "\n",
    "#             print(\"length:\",len(result))\n",
    "#             print(\"1D is:\",result)\n",
    "            max_value = np.amax(result)\n",
    "#             print('Max Value from  Array : ', max_value)\n",
    "            if (max_value >= 0.90 and mark_line[x]== 0):\n",
    "                mark_line[x] = 1\n",
    "                x = x+1\n",
    "                mark_org_index[x] = 1\n",
    "#                 print(\"mark line:\",mark_line,\"\\n\")\n",
    "                count_copy = count_copy + 1\n",
    "            else:\n",
    "                x = x+1\n",
    "\n",
    "#         print(\"No of Lines copied:\" , count_copy)\n",
    "        copy_line.append(count_copy)\n",
    "        \n",
    "        #......Calculate  plagiarism in with train doc..........\n",
    "        total_line = len_np_sent2\n",
    "        copied_line = count_copy\n",
    "        plagiarism = (copied_line/total_line)*100\n",
    "#         print(\"Plagiarism Result is: \",plagiarism,\"%\")\n",
    "        plagiarism_percent.append(plagiarism)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Plagiarism Found :  56.52173913043478 %\n"
     ]
    }
   ],
   "source": [
    "#........calculate total plagiarism of test data...........\n",
    "total_line = len_np_sent2\n",
    "copied_line = 0\n",
    "\n",
    "for x in range(len(mark_line)):\n",
    "    if mark_line[x] == 1:\n",
    "        copied_line = copied_line + 1\n",
    "        \n",
    "plagiarism = (copied_line/total_line)*100\n",
    "print(\"Total Plagiarism Found : \",plagiarism,\"%\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydoc = docx.Document()\n",
    "other_source = 0\n",
    "line_count = 0\n",
    "\n",
    "for i in range(len(mark_org_index)):\n",
    "    if i == 0:\n",
    "        first = mydoc.add_paragraph(org_sent[i])\n",
    "        \n",
    "    elif mark_org_index[i]==0:    \n",
    "        pera = first.add_run(org_sent[i]+\"।\")\n",
    "        \n",
    "    else:\n",
    "        pera = first.add_run(org_sent[i]+\"।\")\n",
    "        font = pera.font\n",
    "        font.color.rgb = RGBColor(255, 0, 0)\n",
    "\n",
    "        \n",
    "first = mydoc.add_paragraph(\"Plagiarim Result:\")\n",
    "\n",
    "for book in range(len(book_name)):\n",
    "    if plagiarism_percent[book]>5.0:\n",
    "        pera = mydoc.add_paragraph().add_run(book_name[book]+\" : \"+str(plagiarism_percent[book])+\"% || \"+str(copy_line[book])+\" Lines Copied\")\n",
    "        font = pera.font\n",
    "        font.color.rgb = RGBColor(0, 51, 102)\n",
    "    else:\n",
    "        others_source = other_source + plagiarism_percent[book]\n",
    "        line_count = line_count + copy_line[book]\n",
    "\n",
    "pera = mydoc.add_paragraph().add_run(\"Others Source : \"+str(others_source)+\"% ||\"+str(line_count))\n",
    "font = pera.font\n",
    "font.color.rgb = RGBColor(0, 51, 102)\n",
    "pera = mydoc.add_paragraph().add_run(\"Total Plagiarism : \"+str(plagiarism)+\"%\")\n",
    "font = pera.font\n",
    "font.color.rgb = RGBColor(0, 51, 102)\n",
    "        \n",
    "mydoc.save(\"report.docx\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
